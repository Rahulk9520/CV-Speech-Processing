{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker recognition using GMMs\n",
    "\n",
    "**Objectives:**\n",
    "* Implementing a speaker recognition algorithm using GMMs\n",
    "* Practising with MFCC features to characterise speakers voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building GMMs with MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build GMMs with MFCC features for a speaker recognition application. In the SpeakerData folder, there are two folders: Train and Test containing training and test speech files respectively. Each Train/Test folder includes twenty-five (25) sub-folders containing speech sounds made by 25 different speakers. There are ten (10) speech files per speaker, in which three (3) files are used for training and the remainders are used for testing. Not any file is used for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs MFCC feature extraction from audio data and builds GMMs for all the speakers in the SpeakerData dataset using MFCC features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "from sklearn import preprocessing\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_extraction(audio_filename, #.wav filename\n",
    "                    hop_duration, #hop_length in seconds, e.g., 0.015s (i.e., 15ms)\n",
    "                    num_mfcc #number of mfcc features\n",
    "                   ):\n",
    "    speech = AudioSegment.from_wav(audio_filename) #Read audio data from file\n",
    "    samples = speech.get_array_of_samples() #samples x(t)\n",
    "    sampling_rate = speech.frame_rate #sampling rate f\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        np.float32(samples),\n",
    "        sr = sampling_rate,\n",
    "        hop_length = int(sampling_rate * hop_duration),\n",
    "        n_mfcc = num_mfcc)\n",
    "    \n",
    "    return mfcc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def learningGMM(features, #list of feature vectors, each feature vector is an array\n",
    "                n_components, #the number of components\n",
    "                max_iter #maximum number of iterations\n",
    "               ):\n",
    "    gmm = GaussianMixture(n_components = n_components, max_iter = max_iter)\n",
    "    gmm.fit(features)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To build GMMs for speakers, we need to define speakers and load their training data. Since each speaker has a folder with their name in the Train/Test folder, the list of speakers can be loaded from the list of sub-folders in the Train/Test folder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asalkeld', 'Bae', 'Azmisov', 'Bachroxx', 'Artk', 'Artem', 'Bahoke', 'Argail', 'Arthur', 'Beez', 'Ara', 'Beady', 'AppleEater', 'Ariyan', 'B', 'Bart', 'Arun', 'Arvala', 'Asp', 'Bassel', 'Asladic', 'Arjuan', 'Bareford', 'BelmontGuy', 'Anthony']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'SpeakerData/'\n",
    "speakers = os.listdir(path + 'Train/')\n",
    "print(speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we load the training data for each speaker and extract the MFCC features from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#this list is used to store the MFCC features of all training data of all speakers\n",
    "mfcc_all_speakers = []\n",
    "\n",
    "hop_duration = 0.015 #15ms\n",
    "num_mfcc = 12\n",
    "\n",
    "for s in speakers:\n",
    "    sub_path = path + 'Train/' + s + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    mfcc_one_speaker = np.asarray(())\n",
    "    for fn in sub_file_names:\n",
    "        mfcc_one_file = mfcc_extraction(fn, hop_duration, num_mfcc)\n",
    "        if mfcc_one_speaker.size == 0:\n",
    "            mfcc_one_speaker = mfcc_one_file\n",
    "        else:\n",
    "            mfcc_one_speaker = np.vstack((mfcc_one_speaker, mfcc_one_file))\n",
    "            \n",
    "    mfcc_all_speakers.append(mfcc_one_speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving MFCC features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As feature extraction is time consuming, we should save the features to files; each file stores the MFCC features extracted from the speech data of one speaker. Suppose that all the features are stored in a folder named TrainingFeatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for i in range(0, len(speakers)):\n",
    "    with open('TrainingFeatures/' + speakers[i] + '_mfcc.fea','wb') as f:\n",
    "        pickle.dump(mfcc_all_speakers[i], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now build our GMMs using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "max_iter = 50\n",
    "gmms = [] #list of GMMs, each is for a speaker\n",
    "for i in range(0, len(speakers)):\n",
    "    gmm = learningGMM(mfcc_all_speakers[i],\n",
    "            n_components,\n",
    "            max_iter)\n",
    "    gmms.append(gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also save the GMMs to files. All the GMMs are stored in a folder named Models, the GMM for each speaker is saved in one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(speakers)):\n",
    "    with open('Models/' + speakers[i] + '.gmm', 'wb') as f: #'wb' is for binary write\n",
    "        pickle.dump(gmms[i], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Speaker recognition using GMMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, we will use the trained GMMs to build a speaker recognition algorithm. We first load the GMMs from files using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmms = []\n",
    "for i in range(len(speakers)):\n",
    "    with open('Models/' + speakers[i] + '.gmm', 'rb') as f: #'wb' is for binary write\n",
    "        gmm = pickle.load(f)\n",
    "        gmms.append(gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MFCC features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_features = []\n",
    "for i in range(len(speakers)):\n",
    "    with open('TrainingFeatures/' + speakers[i] + '_mfcc.fea', 'rb') as f: #'wb' is for binary write\n",
    "        mfcc = pickle.load(f)\n",
    "        mfcc_features.append(mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are required to implement a speaker recognition method named speaker_recognition. This method receives input as a speech file name and a list of GMMs, and returns the ID of the speaker who most likely made the input speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_duration = 0.015 #15ms\n",
    "num_mfcc = 12\n",
    "\n",
    "def speaker_recognition(audio_file_name, gmms):\n",
    "    scores = []\n",
    "    for i in range(len(gmms)):\n",
    "        f = mfcc_extraction(audio_file_name, #.wav filename\n",
    "                        hop_duration, #hop_length in seconds, e.g., 0.015s (i.e., 15ms)\n",
    "                        num_mfcc #number of mfcc features\n",
    "                        )\n",
    "        scores.append(gmms[i].score(f))\n",
    "    speaker_id = scores.index(max(scores))\n",
    "    return speaker_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To identify the speaker of a given a speech sound, e.g., SpeakerData/Test/Ara/a0522.wav, we perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ara\n"
     ]
    }
   ],
   "source": [
    "speaker_id = speaker_recognition('SpeakerData/Test/Ara/a0522.wav', gmms)\n",
    "print(speakers[speaker_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for folder_name in sorted(glob.glob('SpeakerData/Test/*')):\n",
    "    for file_name in sorted(glob.glob(folder_name+\"/*\")):\n",
    "        speaker_id = speaker_recognition(file_name, gmms)\n",
    "        predicted_label = speakers[speaker_id]\n",
    "        true_label = folder_name.split('/')[-1]\n",
    "        pred_labels.append(predicted_label)\n",
    "        true_labels.append(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 3 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(true_labels, prediction_labels)\n",
    "\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Asalkeld       1.00      1.00      1.00         7\n",
      "         Bae       1.00      1.00      1.00         7\n",
      "     Azmisov       1.00      1.00      1.00         7\n",
      "    Bachroxx       1.00      1.00      1.00         7\n",
      "        Artk       1.00      1.00      1.00         7\n",
      "       Artem       1.00      1.00      1.00         7\n",
      "      Bahoke       1.00      0.43      0.60         7\n",
      "      Argail       1.00      1.00      1.00         7\n",
      "      Arthur       0.39      1.00      0.56         7\n",
      "        Beez       1.00      1.00      1.00         7\n",
      "         Ara       1.00      1.00      1.00         7\n",
      "       Beady       1.00      1.00      1.00         7\n",
      "  AppleEater       1.00      1.00      1.00         7\n",
      "      Ariyan       1.00      1.00      1.00         7\n",
      "           B       0.88      1.00      0.93         7\n",
      "        Bart       1.00      0.29      0.44         7\n",
      "        Arun       1.00      1.00      1.00         7\n",
      "      Arvala       1.00      1.00      1.00         7\n",
      "         Asp       1.00      1.00      1.00         7\n",
      "      Bassel       0.70      1.00      0.82         7\n",
      "     Asladic       1.00      1.00      1.00         7\n",
      "      Arjuan       1.00      1.00      1.00         7\n",
      "    Bareford       1.00      1.00      1.00         7\n",
      "  BelmontGuy       1.00      1.00      1.00         7\n",
      "     Anthony       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.91       175\n",
      "   macro avg       0.96      0.91      0.90       175\n",
      "weighted avg       0.96      0.91      0.90       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(true_labels, prediction_labels, labels=speakers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
